import requests
from bs4 import BeautifulSoup
import csv
import time
# using BeautifulSoup we are fetching the data of the products.
def get_product_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    #helps to find title of product
    title = soup.find('span', {'id': 'productTitle'})
    if title:
        title = title.get_text().strip()
    else:
        title = 'N/A'
    #helps to find price of product
    price = soup.find('span', {'id': 'priceblock_ourprice'})
    if price is None:
        price = soup.find('span', {'id': 'priceblock_dealprice'})
    if price is None:
        price = 'N/A'
    else:
        price = price.get_text().strip()
    #helps to find rating of product
    rating = soup.find('span', {'class': 'a-icon-alt'})
    if rating is None:
        rating = 'N/A'
    else:
        rating = rating.get_text().split()[0]
    #helps to find reviews of product
    reviews = soup.find('span', {'id': 'acrCustomerReviewText'})
    if reviews:
        reviews = reviews.get_text().split()[0]
    else:
        reviews = 'N/A'
    #helps to find description of product
    description = soup.find('div', {'id': 'productDescription'})
    if description:
        description = description.get_text().strip()
    else:
        description = 'N/A'
    #returning the details of product
    return {
        'Title': title,
        'Price': price,
        'Rating': rating,
        'Reviews': reviews,
        'Description': description
    }

def main():
    keyword = input("Enter a keyword to search on Amazon: ")
    #totoal results should be 2000
    num_products = 2000
    #we need to do pagination, so for page 48 products.
    products_per_page = 48
    num_pages = (num_products + products_per_page - 1) // products_per_page
    #creating lsit that stores products
    products = []

    for page in range(1, num_pages + 1):
        search_url = f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}&page={page}"
        response = requests.get(search_url)
        soup = BeautifulSoup(response.content, 'html.parser')

        for result in soup.find_all('div', {'data-asin': True}):
            asin = result['data-asin']
            product_url = f"https://www.amazon.com/dp/{asin}"
            product_data = get_product_data(product_url)
            products.append(product_data)

            if len(products) >= num_products:
                break

        if len(products) >= num_products:
            break

        time.sleep(2)  # Add a delay to avoid overloading the server

    # Save data to CSV
    with open('amazon_products.csv', 'w', newline='', encoding='utf-8') as csv_file:
        fieldnames = ['Title', 'Price', 'Rating', 'Reviews', 'Description']
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

        writer.writeheader()
        for product in products:
            writer.writerow(product)
    #details of products are stored to amazon_products.csv file 
    print(f"{len(products)} products data saved to amazon_products.csv")
    #above statement prints total products stored in amazon_products.csv file
if __name__ == "__main__":
    main()
